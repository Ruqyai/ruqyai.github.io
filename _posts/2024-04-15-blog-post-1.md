---
title: 'Demystifying Data Science with AI: A Hands-on Guide Using RAG and Gemma'
date: 2024-04-15
permalink: /posts/2024/04/blog-post-1/
tags:
  - Gemma
  - RAG
  - LLM
---

## Demystifying Data Science with AI: A Hands-on Guide Using RAG and Gemma

The world of data science can be daunting for newcomers, filled with complex terminology and intricate concepts.  But what if you had an AI assistant by your side, ready to explain these concepts in simple terms and guide you through the learning process? This is where the power of Retrieval Augmented Generation (RAG) comes into play.

In this blog post, we'll embark on a hands-on journey, building an AI-powered explainer for data science concepts using the RAG model and the Gemma language model.  

**Why RAG and Gemma?**

RAG is a powerful technique that combines a retriever model (to find relevant information) with a generator model (to create human-like text). This makes it ideal for tasks like explaining complex topics, as it can access and process vast amounts of information and then present it in a clear, understandable way.

We've chosen Gemma-1.1-2b-it, a language model developed by Google AI, as our generator. Gemma is known for its accuracy, efficiency, and ease of use, making it a perfect companion for our RAG system.

<!-- **Building Our AI Explainer: A Step-by-Step Guide**

1. **Gathering the Tools:** We begin by installing the necessary Python libraries: transformers, accelerate, bitsandbytes, langchain, sentence-transformers, chromadb, gradio, and huggingface_hub.

2. **Loading the Knowledge Base:** We use the WebBaseLoader to fetch and process data from the Data Science Glossary, creating a rich repository of data science terms and definitions.

3. **Preparing the Data:**  To optimize processing, we split the glossary entries into smaller chunks using the RecursiveCharacterTextSplitter.

4. **Creating a Vector Database:**  We leverage SentenceTransformer to convert text chunks into numerical representations (embeddings) and store them in a Chroma vector database, enabling efficient similarity search.

5. **Setting Up the Retriever:**  We configure the Chroma database to act as a retriever, allowing us to find the most relevant information based on user queries. 

6. **Bringing in the Language Model:** With the help of Hugging Face, we load the Gemma-1.1-2b-it model, the brainpower behind our explainer's text generation capabilities. 

7. **Building the Conversational Chain:**  We combine the retriever and the language model into a ConversationalRetrievalChain. This chain enables our system to understand user questions, retrieve relevant information, and generate comprehensive answers.

8. **Defining the Conversation Flow:**  We create a function to manage the interaction between the user and the AI system, keeping track of the conversation history and generating responses.

9. **Testing the Explainer:** We put our AI explainer to the test by asking it questions about various data science concepts, such as "What is data science?" and "What is correlation?"

10. **Creating an Interface:** Finally, we use Gradio to build a user-friendly web interface, allowing anyone to interact with our AI explainer and explore the world of data science.

**Benefits and Beyond:**

This RAG-powered explainer offers several benefits:

* **Accessibility:** It makes data science concepts easier to understand for people with different backgrounds and learning styles.
* **Efficiency:**  It provides quick and accurate answers to questions, saving time and effort compared to manual searches.
* **Interactivity:** The user interface allows for a more engaging and personalized learning experience. 

## From Code to Content: Transforming your RAG Notebook into a Captivating Blog Post

**Kudos on your impressive RAG implementation with Gemma!**  Let's turn your technical prowess into an engaging blog post that will captivate data science enthusiasts and beginners alike.

**Structuring the Blog Post:**

1. **Hooking Introduction:**
    * Start with a relatable scenario: Imagine having an AI assistant to explain data science concepts.
    * Briefly introduce RAG and Gemma, highlighting their roles in simplifying complex topics.
    * Mention the goal: Building an AI-powered explainer for data science.

2. **Why RAG and Gemma?**
    * Explain the power of RAG in combining retrieval and generation for comprehensive explanations. 
    * Emphasize Gemma's suitability due to its accuracy, efficiency, and user-friendliness.

3. **Building the AI Explainer: A Step-by-Step Guide**
    * Divide this section into clear steps, mirroring your notebook's structure:
        * **Gathering the Tools:** Briefly list the required libraries.
        * **Loading the Knowledge Base:** Describe using `WebBaseLoader` and the data source (Data Science Glossary).
        * **Preparing the Data:** Explain the text splitting process with `RecursiveCharacterTextSplitter`.
        * **Creating a Vector Database:**  Introduce SentenceTransformer and Chroma, emphasizing their role in efficient search.
        * **Setting Up the Retriever:** Explain how Chroma acts as a retriever to find relevant information.
        * **Bringing in the Language Model:** Describe loading Gemma using Hugging Face.
        * **Building the Conversational Chain:** Explain the purpose of `ConversationalRetrievalChain` in connecting the retriever and LLM. 
        * **Defining the Conversation Flow:** Briefly mention the function for managing user interaction and generating responses.
        * **Testing the Explainer:** Share examples of questions and answers to showcase the model's capabilities.
        * **Creating an Interface:** Highlight the use of Gradio to build a user-friendly web interface.


## Why I Used RAG for the Competition

I chose to use **Retrieval Augmented Generation (RAG)** to explain basic data science concepts for several reasons:

**1. Ease of use:** RAG is a relatively easy-to-use model, making it a good choice for beginners in natural language processing (NLP).

**2. Accuracy:** RAG has shown to be accurate on text generation tasks, including explanation and summarization.

**3. Adaptability:** RAG can be adapted to a variety of NLP tasks, including explaining scientific concepts.

**4. Computational efficiency:** RAG is a computationally efficient model, making it a good choice for use on devices with limited resources.

**Advantages of using RAG:**

* **Accurate explanations:** RAG provides accurate explanations of scientific concepts.

* **Ease of use:** RAG is relatively easy to use, making it a good choice for beginners.

* **Adaptability:** RAG can be adapted to a variety of NLP tasks.

* **Computational efficiency:** RAG is a computationally efficient model.

## Why I Used Gemma-1.1-2b-it
**Key Advantages of Gemma-1.1-2b-it**

* **Relatively Compact Size:** Compared to other massive LLMs, Gemma 1.1-2b-it is smaller. This translates to easier deployment on devices with more modest computational resources.

* **Accuracy:** The model demonstrates strong accuracy across various natural language processing tasks. This means you can expect reliable and correct responses.

* **Adaptability:** Gemma 1.1-2b-it has the flexibility to be fine-tuned for specific domains and tasks, enhancing its performance in those areas.

* **Computational Efficiency:** This model is designed to process large volumes of text data quickly and efficiently.

* **Ease of Use:**  Gemma 1.1-2b-it  is relatively user-friendly, allowing even those with less NLP experience to utilize it effectively.

**Specific Examples of Gemma 1.1-2b-it's Capabilities**

* **Question Answering:**  Can tackle a broad spectrum of questions, including complex, open-ended, or unusual ones.
* **Summarization:**  Skillfully generates concise summaries of lengthy text passages.
* **Translation:**  Provides translation services between different languages.
* **Code Generation** Gemma can generate code, potentially aiding software development. 
* **Creative Text Formats** Can produce various creative content like poetry, scripts, or even musical pieces.

**Additional Considerations**

* **Open-Source:**  Gemma's open-source nature promotes wider use,  modification, and collaborative development. 
* **Strong Documentation:** Excellent documentation simplifies the process of getting started with the model.
* **Active Community Support:**  Benefit from a vibrant community of researchers dedicated to Gemma, offering support and resources. -->
---
---

# Retrieval Augmented Generation (RAG) using Gemma to explain basic data science concepts. 
In this tutorial, we'll use the Retrieval Augmented Generation (RAG) model to explain basic data science concepts. RAG combines a retriever and a language model to provide relevant and accurate responses to questions.
![Retrieval Augmented Generation (RAG).png](https://i.ibb.co/VwHwkw7/Retrieval-Augmented-Generation-RAG.png)

# Step 1: Install Required Packages
First, let's install the necessary packages using pip:

```bash
pip install transformers accelerate bitsandbytes langchain sentence-transformers chromadb gradio huggingface_hub
```
### Note: You must restart your notebook to avoid Gradio package errors

# Step 2: Import Required Libraries
Next, let's import the libraries we'll need:

```python
import os
import gradio as gr
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.llms import HuggingFaceEndpoint
```

# Step 3: Load Data

Let's load the data from a web-based source. In this example, we'll use a data science glossary:

```python
loader = WebBaseLoader("https://www.datascienceglossary.org/")
data = loader.load()
```
**Code Explanation:**

* **Create a loader object designed to fetch and process data from the Data Science Glossary:**
   `loader = WebBaseLoader("https://www.datascienceglossary.org/")`

* **Call the loader's 'load' method. This likely initiates the following steps:**
   1. **Fetching the website's content**
   2. **Extracting relevant data (e.g., definitions, term lists)**
   3. **Structuring the extracted data**
   `data = loader.load()`

# Step 4: Split Documents
To improve efficiency, we'll split the documents into smaller chunks:

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
splits = text_splitter.split_documents(data)
```
**Code Explanation:**

**Purpose:**  The goal of this code is to break down larger text documents into smaller, more manageable chunks. This is often done for efficiency in natural language processing tasks where models might have limitations on how much text they can process at once.

* **`text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)`:** 
   - This line creates an object called `text_splitter` which belongs to the class `RecursiveCharacterTextSplitter`. This class is designed to split text into chunks.
   - `chunk_size=500`:  This parameter controls the desired maximum size of each text chunk. Here, chunks will be approximately 500 characters long.
   - `chunk_overlap=0`: This parameter sets the amount of overlap between chunks. Here, there won't be any overlap.

* **`splits = text_splitter.split_documents(data)`:**
    -  This line takes the list of documents (which we assume is stored in the  `data` variable) and applies the text splitting logic defined earlier.     
    -  The result is stored in the `splits` variable, likely as a new list where each element is a smaller text chunk.

# Step 5: Create Vector Database
We'll use SentenceTransformer to embed the text and create a vector database:
```python
embedding = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')
vectordb = Chroma.from_documents(documents=splits, embedding=embedding)
```
**Code Explanation:**

**Purpose** : This step transforms textual data into numerical vectors (embeddings) and stores them in a specialized database designed for quick searches and similarity comparisons.

**1. SentenceTransformerEmbeddings**

* **`embedding = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')`**
    * Creates an object named `embedding` responsible for generating text embeddings.
    * It uses a pre-trained Sentence Transformers model called 'all-MiniLM-L6-v2'. This model converts sentences/paragraphs into numerical vectors.

**2. Chroma Vector Database**

* **`vectordb = Chroma.from_documents(documents=splits, embedding=embedding)`**
    * Creates a vector database named `vectordb` using the Chroma library.
    * `documents=splits`: Uses the previously created text chunks (`splits`) as input.
    * `embedding=embedding`:  Specifies the embedding model to generate vectors for each text chunk.

# Step 6: Create Retriever
Now, we'll create a retriever using the vector database:
```python
retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 2})
```
**Code Explanation:**

**Purpose**: This step sets up a mechanism to search through the vector database you created earlier. The retriever will help you find text chunks that are semantically similar to a given query.

* **`retriever = vectordb.as_retriever(...)`:**
    * This line creates a `retriever` object using the `vectordb` (your Chroma vector database). 
    * The `.as_retriever()` method of Chroma configures how the database will be searched.

* **`search_type="similarity"`:**
    * This parameter tells the retriever that you'll be searching for items in the database based on their similarity to a query (as opposed to searching by exact matches or other criteria).

* **`search_kwargs={"k": 2}`:**
    * This provides additional search settings. Here, the key-value pair `k:2` instructs the retriever to return the 2 most similar items from the database for a given query.

# Step 7: Load Language Model
We'll use a pre-trained language model from Hugging Face:
```python
os.environ["HUGGINGFACEHUB_API_TOKEN"] = 'API_TOKEN'

repo_id = "google/gemma-1.1-2b-it"

llm = HuggingFaceEndpoint( repo_id=repo_id, max_length=1024, temperature=0.1)
```
**Code Explanation:**

**Purpose**: This step loads a powerful pre-trained language model (LLM) called "Gemma 1.1-2b-it" from the Hugging Face platform. This model will be the core of your application's ability to generate text, answer questions, and perform other language-related tasks.

1. **Setting API Token**
   * `os.environ["HUGGINGFACEHUB_API_TOKEN"] = 'API_TOKEN'` 
      * This line likely retrieves a secret Hugging Face API token required to access and download the model.  

2. **Specifying Model ID**
   * `repo_id = "google/gemma-1.1-2b-it"`
      * This defines the unique identifier of the model on the Hugging Face platform. This tells the system to load the Gemma 1.1-2b-it model developed by Google.

3. **Loading the Model**
   *  `llm = HuggingFaceEndpoint(repo_id=repo_id, max_length=1024, temperature=0.1)`
      * Creates an object named `llm` of the type `HuggingFaceEndpoint`. This object provides an interface to interact with the loaded language model.
      * `max_length=1024`:  Sets a limit on the maximum number of tokens (roughly words or word parts) the model can process at once.
      * `temperature=0.1`: Controls the randomness in the model's text generation. Lower values lead to more predictable and deterministic output.

# Step 8: Create Conversational Retriever Chain
Combine the retriever and language model into a conversational retriever chain:
```python
qa = ConversationalRetrievalChain.from_llm(llm, retriever)

```
**Code Explanation:**

**Purpose**: This step integrates the vector-based retriever (for finding relevant information) and the language model (for generating text) into a single, powerful component. This chain enables a more natural question-and-answer experience for users.

* **`qa = ...`:** This line creates a new object named `qa` which will represent your conversational retriever chain.

* **`ConversationalRetrievalChain.from_llm(llm, retriever)`:**
    * `ConversationalRetrievalChain`: This class (likely from the `langchain` library) provides the framework for building chains that combine different language models or components.
    * `.from_llm()`: A method to easily construct a chain starting with a language model.
    * `llm`:  This is where you pass your previously loaded language model object.
    * `retriever`: You also provide the retriever object you created earlier.


# Step 9: Define Conversation Execution Function
Define a function to execute the conversation:

```python
def execute_conversation(question):
    chat_history = []
    result = qa({"question": question, "chat_history": chat_history})
    chat_history.append(result["answer"])
    return result["answer"]
```

**Code Explanation:**

**Purpose**: This step defines a function named `execute_conversation` that handles the core logic of interacting with your conversational AI system. It takes a question as input and generates an answer using the retrieval chain you built earlier.


**Step-by-Step Explanation**

1. **`chat_history = []`** 
   *  A list called `chat_history` is created to store the conversation's progression (questions and answers). It starts empty.

2. **`result = qa({"question": question, "chat_history": chat_history})`**
   *  The `qa` object (your `ConversationalRetrievalChain`) is called. 
   * It's provided a dictionary containing the current `question` and the `chat_history`.
   * The chain processes this input and generates a result (which includes the answer).

3. **`chat_history.append(result["answer"])`**
    * The answer generated by the chain (found in `result["answer"]`) is added to the `chat_history` to keep track of the conversation.

4. **`return result["answer"]`**
   *  The function returns the generated answer as its output.

# Step 10: Define Questions and Get Answers
Now, you can define questions and get answers using the execute_conversation function. 
**It is better to choose questions from this page to make sure it works well:**

 ***https://www.datascienceglossary.org/***

```python
question1="What is data science?"
print(execute_conversation(question1))
question2="What is correlation?"
print(execute_conversation(question2))
question3="What is Mean Squared Error?"
print(execute_conversation(question3))

```


# Step 11: Create a Gradio Interface
Finally, create a Gradio interface to interact with the model:  
**Note : It is better to choose questions from this page to make sure it works well.** 
***https://www.datascienceglossary.org/***

```python
chatbot = gr.Interface(
    fn=execute_conversation,
    inputs="text",
    outputs="text",
    live=False,
    title="RAG using Gemma to explain basic data science concepts.",
    description="Enter your question",
)

chatbot.launch()
```
​
**Code Explanation:**
​
**Purpose**: This step creates a Gradio interface to interact with the model.
​
**What is Gradio?**
​
Gradio is a Python library specifically designed to help you quickly create visual web interfaces for your machine learning models. It lets you demonstrate, test, and share your models with others in a user-friendly way.
​
* **`fn=execute_conversation`**:  This is the core of your interface. It tells Gradio that the function `execute_conversation` is responsible for handling user input and generating the output. You would have defined this function earlier in your code to encapsulate your RAG model's logic.
* **`inputs="text"`**:  Specifies that the interface will accept a single text input field where the user can type their question.
* **`outputs="text"`**:  Indicates that the model's response will also be in the form of text.
* **`live=False`**:  This setting means the interface won't update in real-time as the user types. Instead, the model will process the question only after the user submits it. 
* **`title="..."`**: Sets the title of your Gradio interface, which appears in the web browser's tab.
* **`description="..."`**: Provides a brief description or instructions for the user, displayed below the input field in the interface.
​
**How It Works in the RAG Context**
​
1. **User Input:** The user visits the web address where your Gradio interface is running and enters a question about a data science concept in the text box.
2. **Function Call:** When the user submits their question, the `execute_conversation` function is triggered. This function likely handles:
   * **Retrieval:**  Your RAG model retrieves relevant documents or passages from your knowledge base.
   * **Generation:** The model generates a comprehensive answer based on the retrieved information and its understanding of data science concepts.
3. **Output:** The generated response is sent back to the Gradio interface and displayed in the output text area.
​
**Launching the Interface**
​
* **`chatbot.launch()`**: This line starts the Gradio web server and launches the interface in your web browser, making it accessible for interaction. 
![What-is-data-science](https://i.ibb.co/hHCnzr7/What-is-data-science.png)
![Please-explain-what-is-the-clustering](https://i.ibb.co/QrnDBkT/Please-explain-what-is-the-clustering.png)




## Conclusion

In this tutorial, we demonstrated how to use Gemma LLM with RAG to explain basic data science concepts. By combining Gemma's capabilities with RAG's retrieval and generation features, we can create a powerful tool for understanding and explaining complex concepts in a user-friendly manner.

## Check my notebook
[My notebook](https://www.kaggle.com/code/ruqiyas/rag-using-gemma-to-explain-data-science-concepts)

```
@misc{data-assistants-with-gemma,
    author = {Paul Mooney, Ashley Chow},
    title = {Google – AI Assistants for Data Tasks with Gemma},
    publisher = {Kaggle},
    year = {2024},
    url = {https://kaggle.com/competitions/data-assistants-with-gemma}
}
```